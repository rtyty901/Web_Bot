# í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
import os
from dotenv import load_dotenv
load_dotenv()

# langchain íŒ¨í‚¤ì§€
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
import gradio as gr

# RAG Chain êµ¬í˜„ì„ ìœ„í•œ íŒ¨í‚¤ì§€
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser

# ë²¡í„° ì €ì¥ì†Œ ìºì‹±ì„ ìœ„í•œ ì „ì—­ ë³€ìˆ˜
vectorstore_cache = {}


# ì›¹ í˜ì´ì§€ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ì—¬ ë²¡í„° ì €ì¥ì†Œì— ì €ì¥
def load_web_to_vector_store(url, chunk_size=1000, chunk_overlap=200, cache_key=None):
    """
    ì›¹ í˜ì´ì§€ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ì—¬ ë²¡í„° ì €ì¥ì†Œì— ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        url: ì›¹ í˜ì´ì§€ URL
        chunk_size: ì²­í¬ í¬ê¸°
        chunk_overlap: ì²­í¬ ì˜¤ë²„ë© í¬ê¸°
        cache_key: ìºì‹œ í‚¤ (Noneì´ë©´ URL ì‚¬ìš©)
    
    Returns:
        FAISS ë²¡í„° ì €ì¥ì†Œ ì¸ìŠ¤í„´ìŠ¤
    """
    try:
        # ìºì‹œ í‚¤ ìƒì„±
        if cache_key is None:
            cache_key = f"{url}_{chunk_size}_{chunk_overlap}"
        
        # ìºì‹œ í™•ì¸
        if cache_key in vectorstore_cache:
            return vectorstore_cache[cache_key]
        
        # URL ìœ íš¨ì„± ê²€ì‚¬
        if not url or not url.strip():
            raise ValueError("URLì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url.strip()
        
        # ì›¹ í˜ì´ì§€ ë¡œë”©
        print(f"ì›¹ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ì¤‘: {url}")
        loader = WebBaseLoader(url)
        documents = loader.load()
        
        if not documents:
            raise ValueError("ì›¹ í˜ì´ì§€ì—ì„œ ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        splits = text_splitter.split_documents(documents)
        
        # FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„± ë° ë¬¸ì„œ ì„ë² ë”©ìœ¼ë¡œ ì´ˆê¸°í™”
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)
        
        # ìºì‹œì— ì €ì¥
        vectorstore_cache[cache_key] = vectorstore
        
        return vectorstore
    except Exception as e:
        raise Exception(f"ì›¹ ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


# ë²¡í„° ì €ì¥ì†Œì—ì„œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  ë‹µë³€ì„ ìƒì„±
def retrieve_and_generate_answers(vectorstore, message, temperature=0):
    """
    ë²¡í„° ì €ì¥ì†Œì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  RAGë¥¼ í†µí•´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        vectorstore: FAISS ë²¡í„° ì €ì¥ì†Œ ì¸ìŠ¤í„´ìŠ¤
        message: ì‚¬ìš©ì ì§ˆë¬¸
        temperature: ëª¨ë¸ ì˜¨ë„ ì„¤ì •
    
    Returns:
        ìƒì„±ëœ ë‹µë³€ ë¬¸ìì—´
    """
    try:
        if not message or not message.strip():
            return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
        
        # RAG ì²´ì¸ ìƒì„±
        retriever = vectorstore.as_retriever()

        # Prompt
        template = '''Answer the question based only on the following context from the web page:
<context>
{context}
</context>

Question: {input}

Provide a clear and concise answer based only on the provided context. If the context doesn't contain enough information to answer the question, say so.'''

        prompt = ChatPromptTemplate.from_template(template)

        # ChatModel ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        model = ChatOpenAI(model='gpt-4o-mini', 
                           temperature=temperature)

        # LCELì„ ì‚¬ìš©í•œ RAG ì²´ì¸ êµ¬ì„±
        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)
        
        rag_chain = (
            {"context": retriever | RunnableLambda(format_docs), "input": RunnablePassthrough()}
            | prompt
            | model
            | StrOutputParser()
        )

        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±
        answer = rag_chain.invoke(message)

        return answer
    except Exception as e:
        return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


# Gradio ì¸í„°í˜ì´ìŠ¤ì—ì„œ ì‚¬ìš©í•  í•¨ìˆ˜
def process_web_and_answer(message, history, url, chunk_size, chunk_overlap, temperature):
    """
    ì›¹ í˜ì´ì§€ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ê³  ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ì…ë‹ˆë‹¤.
    
    Args:
        message: ì‚¬ìš©ì ì§ˆë¬¸
        history: ëŒ€í™” ê¸°ë¡
        url: ì›¹ í˜ì´ì§€ URL
        chunk_size: ì²­í¬ í¬ê¸°
        chunk_overlap: ì²­í¬ ì˜¤ë²„ë©
        temperature: ëª¨ë¸ ì˜¨ë„
    
    Returns:
        ìƒì„±ëœ ë‹µë³€
    """
    try:
        # URLì´ ì—†ìœ¼ë©´ ì˜¤ë¥˜ ë©”ì‹œì§€ ë°˜í™˜
        if url is None or not url.strip():
            return "ì›¹ í˜ì´ì§€ URLì„ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”."
        
        # ì…ë ¥ ê°’ ê²€ì¦
        chunk_size = int(chunk_size) if chunk_size else 1000
        chunk_overlap = int(chunk_overlap) if chunk_overlap else 200
        temperature = float(temperature) if temperature else 0.0
        
        # ìœ íš¨ì„± ê²€ì‚¬
        if chunk_size <= 0:
            return "Chunk SizeëŠ” 0ë³´ë‹¤ ì»¤ì•¼ í•©ë‹ˆë‹¤."
        if chunk_overlap < 0:
            return "Chunk Overlapì€ 0 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤."
        if chunk_overlap >= chunk_size:
            return "Chunk Overlapì€ Chunk Sizeë³´ë‹¤ ì‘ì•„ì•¼ í•©ë‹ˆë‹¤."
        if temperature < 0 or temperature > 2:
            return "TemperatureëŠ” 0ê³¼ 2 ì‚¬ì´ì˜ ê°’ì´ì–´ì•¼ í•©ë‹ˆë‹¤."
        
        # URL ì •ê·œí™”
        url = url.strip()
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        # ìºì‹œ í‚¤ ìƒì„± (URL + ì„¤ì •ê°’)
        cache_key = f"{url}_{chunk_size}_{chunk_overlap}"
        
        # ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ (ìºì‹œ ì‚¬ìš©)
        vectorstore = load_web_to_vector_store(
            url, 
            chunk_size, 
            chunk_overlap, 
            cache_key=cache_key
        )

        # ë‹µë³€ ìƒì„±
        answer = retrieve_and_generate_answers(vectorstore, message, temperature)

        return answer
    except ValueError as e:
        return f"ì…ë ¥ ê°’ ì˜¤ë¥˜: {str(e)}"
    except Exception as e:
        return f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


# Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±
demo = gr.ChatInterface(
    fn=process_web_and_answer,
    title="Web Scraping RAG ì±—ë´‡",
    description="""ì›¹ í˜ì´ì§€ URLì„ ì…ë ¥í•˜ê³  ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”!
    
**ì‚¬ìš© ë°©ë²•:**
1. ì•„ë˜ "Additional Inputs" ì„¹ì…˜ì„ í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°
2. "ì›¹ í˜ì´ì§€ URL" ì…ë ¥ë€ì— URL ì…ë ¥ (ì˜ˆ: https://example.com)
3. ì§ˆë¬¸ì„ ì…ë ¥í•˜ê³  ì „ì†¡í•˜ê¸°

**ì§€ì› í˜•ì‹:** 
- ëª¨ë“  HTTP/HTTPS ì›¹ í˜ì´ì§€ URL
- http:// ë˜ëŠ” https:// ì—†ì´ ì…ë ¥í•´ë„ ìë™ìœ¼ë¡œ ì¶”ê°€ë©ë‹ˆë‹¤""",
    additional_inputs=[
        gr.Textbox(label="ğŸŒ ì›¹ í˜ì´ì§€ URL (í•„ìˆ˜)", 
                   placeholder="https://example.com ë˜ëŠ” example.com",
                   value=""),
        gr.Number(label="Chunk Size", value=1000, minimum=100, maximum=5000, step=100),
        gr.Number(label="Chunk Overlap", value=200, minimum=0, maximum=1000, step=50),
        gr.Slider(label="Temperature", minimum=0, maximum=2, step=0.1, value=0.0),
    ],
    examples=[
        ["ì´ ì›¹í˜ì´ì§€ì˜ ì£¼ìš” ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?", "https://www.python.org", 1000, 200, 0.0],
        ["í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.", "https://www.python.org", 1000, 200, 0.0],
    ],
)

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰
if __name__ == "__main__":
    # API í‚¤ í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸ ê²½ê³ : OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    
    demo.launch(share=False)
